<h1 align="center"> Pandas ETL Challenge </h1>

![pandas logo](https://user-images.githubusercontent.com/93624837/203309606-0b034b69-c40e-4ee6-9029-019676950f1c.jpg)

![badge release date](https://img.shields.io/badge/release%20date-November%2F2022-blue)

### TOPICS

* [Code Description](#code-description)
* [My Comments](#my-comments)
* [Author](#author)

## Code Description

<p align="justify">
This is a ETL pipeline created using Python and his library Pandas. This script ("main.py") it's a improved version of the original script ("exercise.py"):  it's read a CSV file ("source_data.csv"), then the data contained in this file are processed and in the end, a new CSV file ("processed_data.csv") it's created.
</p>

## My Comments

<p align="justify">
With the small information who was given about this task, I decide to keep some aspects of the original script (like use the minimum of libraries, keep all the code in a single file and etc.) and use it as a guidelines when I wrote this code.

In the code, I try to keep it as clean, simple and readable as possible. 

In my humble opinion (based on my professional experience), the best code it's the code who fits the bussiness proccess and needs. For example, I could use Great Expectations library to validate the data from the source CSV file ("source_data.csv") in order to improve the data integrity, but I assumed that this would be not necessary, based on the steps that was used in the original script ("exercise.py").

In order to improve this script even more and provides high quality data, it's very importante to deep understand the bussiness and IT proccess about this required data. Without this, sometimes it's better keep the code simple than use "a cannon to kill a ant".
  
</p>


## Author

<p align="justify"> Gustavo de Souza Pessanha da Costa. </p>


